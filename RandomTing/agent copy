import random
import numpy as np
import torch

class Q_Agent:
    def __init__(self, env, model):
        self.env = env
        self.epsilon = 0.3
        self.model = model
        self.optim = torch.optim.Adam(model.parameters())
        self.loss = torch.nn.MSELoss()

    def get_action(self, state):
        ava_actions = self.env.ava_actions()
        red_adj = self.preprocces_state(state)
        q_values = self.model(red_adj)
        if random.random() < self.epsilon:
            action = random.choice(ava_actions)
        else:
            action = torch.argmax(q_values[ava_actions]).item()
        q_value = q_values[action]
        return action, q_value

    def q_step(self, q_value, reward, next_state):
        if self.env.is_solution():
            target = torch.Tensor([reward])
        else:
            ava_actions = self.env.ava_actions()
            target = torch.Tensor([reward]) + torch.max(self.model(next_state)[ava_actions])
        error = self.loss(q_value.flatten(), target.flatten())
        self.optim.zero_grad()
        error.backward()
        self.optim.step()
        return target, error

    def train(self):
        is_solution = False
        state = self.env.get_state()
        while not is_solution:
            action, q_value = self.get_action(state)
            next_state, reward, is_solution, _ = self.env.step(action)
            self.q_step(q_value, reward, next_state)
            state = next_state
        return state


    def preprocces_state(self, state):
        adj = self.env.get_instace()

